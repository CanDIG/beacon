---
layout: default
---

# Beacon Design

Early plans had Beacon as just a simple layer on top of CanDIG.
Unfortunately, CanDIG's documented API did not provide the necessary
power for filtering/searching through variant data. As such, Beacon
had to be designed to perform these searches all on its own.

Beacon was initially scaffolded out of [openapi-generator][].

[openapi-generator]: https://openapi-generator.tech
[golang gin generator]: https://openapi-generator.tech/docs/generators/go-experimental

## Repo Layout

The repository has been laid out with each of the relevant
dependencies listed as sub-modules of the main repo to enable
reproducible building.

- /beacon-server: the server implementation as generated by
  [openapi-generator][]'s [golang gin generator][].
- /candig-client: client for CanDIG generated by
  [openapi-generator][], generated from /candig-server
- /candig-server: version of candig-server that beacon works against
  and follows

## Server

Due to the large amount of data that Beacon might be forced to handle,
keeping memory demands constrained were essential to the design of
this system. A streaming MapReduce architecture was conceived.

This Beacon is designed to extensively make use of Go's concurrency by
using the [pipeline][] pattern. This allows us to stream large amounts
of data out of the database and filter/process it within our Go code
without filling up memory. It also allows us to create well defined
worker pools out of go routines that ensure we don't overload the
system.

[pipeline]: https://blog.golang.org/pipelines

### Index Endpoint

This was just a basic lookup and retrieval of what Datasets were
available and various information about the CanDIG platform that the
Beacon was gatewaying.

### Query Endpoint

This is the real meat of the problem at hand.

#### Architecture

For each Dataset requested (all if none specified), the number of
matching variants are counted using the [pipeline][] pattern
introduced earlier. Within the Dataset, all the available variants
within the given range are enumerated and then filtered out by a
collection of worker go-routines. An extra step of looking up the
Variant's Assembly Id needs to be performed so the number of workers
is set to a large factor of the processor count. After filtering, the
total number of variants that made it through are added up.

As an opportunistic optimization, the set of RefSets matching the
given AssemblyId are fetched. If there are to many of these (exceeding
a fixed maximum), then we ignore them. Then for each Dataset, the
VariantSets that match the RefSets are fetched into memory, but only
if they don't exceed a defined maximum. These VariantSets are passed
along to the Variant listing API to restrict the number of Variants
processed at the source. This also allows us to skip a later call to
the CanDIG API to verify that the Assembly Id of a variant matches the
one provided in the Beacon API request.

#### Future Architectural Changes

A job queue could be implemented to add further scalability to the
filtering pipeline, but this was treated as a premature optimization
as we had already realized significant gains. The per-Variant work was
very minimal in proportion to the size of information, which would
result in a significant network bottleneck for minimal compute
increases.
